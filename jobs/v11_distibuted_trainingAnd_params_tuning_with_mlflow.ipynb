{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distibuted Training With Hyperparameter tuning using Hyperopt and MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/09 09:01:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"distibuted_trainingAnd_params_tuning\").master(\"spark://spark-master:7077\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs for Tuning\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "\n",
    "# for distibuted training\n",
    "from hyperopt import SparkTrials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(l1_noNode,l1_activation,l1_droupout):\n",
    "    model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(l1_noNode, activation=l1_activation),\n",
    "    tf.keras.layers.Dropout(l1_droupout),\n",
    "    tf.keras.layers.Dense(10)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(p_epoch=2 ,p_optimizer=\"adam\" ,\n",
    "                l1_noNode=32,l1_activation=\"relu\",l1_droupout=0.2):\n",
    "\n",
    "\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model = create_model(l1_noNode,l1_activation,l1_droupout)\n",
    "\n",
    "\n",
    "    model.compile(optimizer=p_optimizer,\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "  \n",
    "    model.fit(x_train, y_train, epochs=p_epoch)\n",
    "    eval_loss, eval_acc  = model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "    print(\"eval_loss, eval_acc : \",eval_loss, eval_acc)\n",
    "    return model, eval_loss, eval_acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_hyperopt(params):\n",
    "    with mlflow.start_run(nested=True) as run:\n",
    "        p_epoch = params[\"epochs\"]\n",
    "        p_optimizer = params[\"optimizer\"]\n",
    "        l1_noNode = params[\"l1_noNode\"]\n",
    "        l1_activation= params[\"l1_activation\"]\n",
    "        l1_droupout = params[\"l1_droupout\"]\n",
    "\n",
    "        model, eval_loss, eval_acc = train_model(p_epoch,p_optimizer,l1_noNode,l1_activation,l1_droupout)\n",
    "\n",
    "\n",
    "\n",
    "        best_model, eval_loss, eval_acc = train_model(p_epoch=p_epoch,p_optimizer=p_optimizer,\n",
    "                                                l1_activation=l1_activation,\n",
    "                                                l1_droupout=l1_droupout,\n",
    "                                                l1_noNode=l1_noNode)\n",
    "\n",
    "\n",
    "        mlflow.log_metric(\"eval_loss\",eval_loss)\n",
    "        mlflow.log_metric(\"eval_acc\",eval_acc)\n",
    "        model_signature = infer_signature(x_train,best_model.predict(x_train))\n",
    "        \n",
    "        mlflow.keras.log_model(best_model,\"ths_tune_model\",\n",
    "                               signature=model_signature,\n",
    "                               input_example= x_train[0:5])\n",
    "\n",
    "    return {\"loss\": eval_loss, \"status\": STATUS_OK, \"model\": model, \"eval_acc\": eval_acc}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Because the requested parallelism was None or a non-positive value, parallelism will be set to (4), which is Spark's default parallelism (4), or 1, whichever is greater. We recommend setting parallelism explicitly to a positive value because the total of Spark task slots is subject to cluster sizing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function suggest at 0xffff9a100d60>\n"
     ]
    }
   ],
   "source": [
    "epochs = [1,2,3,4]\n",
    "optimizer = [\"Adam\",\"SGD\",\"RMSprop\"]\n",
    "l1_noNode = [32,64,128] \n",
    "l1_activation = [\"relu\",\"softmax\",\"tanh\"]\n",
    "l1_droupout = [0.2,0.3,0.4,0.5]\n",
    "\n",
    "search_params_space ={\n",
    "    \"epochs\": hp.choice(\"epochs\",epochs),\n",
    "    \"optimizer\" : hp.choice(\"optimizer\",optimizer),\n",
    "    \"l1_noNode\" : hp.choice(\"l1_noNode\",l1_noNode ),\n",
    "    \"l1_activation\" : hp.choice(\"l1_activation\",l1_activation),\n",
    "    \"l1_droupout\": hp.choice(\"l1_droupout\", l1_droupout)\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "spark_trials = SparkTrials()\n",
    "algo = tpe.suggest\n",
    "print(algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define MLflow experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/707538321879479075', creation_time=1746778774364, experiment_id='707538321879479075', last_update_time=1746778774364, lifecycle_stage='active', name='distibuted_trainingAnd_params_tuning_with_mlflow_1', tags={}>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import time\n",
    "mlflow_uri = \"http://mlflow-server:8888/\"\n",
    "mlflow.set_tracking_uri(mlflow_uri)\n",
    "time.sleep(5)\n",
    "mlflow.set_experiment(\"distibuted_trainingAnd_params_tuning_with_mlflow_1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import mlflow.pyspark.ml\n",
    "mlflow.pyspark.ml.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.3'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "mlflow.tensorflow.autolog()\n",
    "mlflow.enable_system_metrics_logging()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/05/09 09:05:33 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh(<full-path-to-git-executable>)\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial message can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|silent|none|n|0: for no message or exception\n",
      "    - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)\n",
      "    - error|e|exception|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n",
      "2025/05/09 09:05:33 INFO mlflow.system_metrics.system_metrics_monitor: Skip logging GPU metrics. Set logger level to DEBUG for more details.\n",
      "2025/05/09 09:05:33 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>    (0 + 1) / 1][Stage 1:>    (0 + 1) / 1][Stage 2:>    (0 + 1) / 1]1]\r"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run() as run:\n",
    "\n",
    "    argmin = fmin(\n",
    "    fn= train_with_hyperopt,\n",
    "    space=search_params_space,\n",
    "    algo=algo,\n",
    "    max_evals = 3,\n",
    "    # for distrubuted training\n",
    "    trials=spark_trials \n",
    "    )\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': 1,\n",
       " 'l1_activation': 0,\n",
       " 'l1_droupout': 0,\n",
       " 'l1_noNode': 2,\n",
       " 'optimizer': 0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained tf model with Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set MLflow backend URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/531492440694294041', creation_time=1746354896611, experiment_id='531492440694294041', last_update_time=1746354896611, lifecycle_stage='active', name='distibuted_trainingAnd_params_tuning_with_mlflow_1', tags={}>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import time\n",
    "mlflow_uri = \"http://mlflow-server:8888/\"\n",
    "mlflow.set_tracking_uri(mlflow_uri)\n",
    "time.sleep(5)\n",
    "mlflow.set_experiment(\"distibuted_trainingAnd_params_tuning_with_mlflow_1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"distibuted_trainingAnd_params_tuning\").master(\"spark://spark-master:7077\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RestException",
     "evalue": "RESOURCE_DOES_NOT_EXIST: Run '9794c5f6d6dc41bf9ae605e2abf80364' not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRestException\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m logged_model = \u001b[33m'\u001b[39m\u001b[33mruns:/9794c5f6d6dc41bf9ae605e2abf80364/ths_tune_model\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load model as a Spark UDF. Override result_type if the model does not return double values.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m loaded_model =  \u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpyfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogged_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/mlflow/tracing/provider.py:422\u001b[39m, in \u001b[36mtrace_disabled.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    420\u001b[39m disable()\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m     is_func_called, result = \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    424\u001b[39m     enable()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/mlflow/pyfunc/__init__.py:1099\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(model_uri, suppress_warnings, dst_path, model_config)\u001b[39m\n\u001b[32m   1095\u001b[39m         entity_list.append(Entity(job=job_entity))\n\u001b[32m   1097\u001b[39m     lineage_header_info = LineageHeaderInfo(entities=entity_list) \u001b[38;5;28;01mif\u001b[39;00m entity_list \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1099\u001b[39m local_path = \u001b[43m_download_artifact_from_uri\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1100\u001b[39m \u001b[43m    \u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdst_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlineage_header_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineage_header_info\u001b[49m\n\u001b[32m   1101\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m suppress_warnings:\n\u001b[32m   1104\u001b[39m     model_requirements = _get_pip_requirements_from_model_path(local_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/mlflow/tracking/artifact_utils.py:108\u001b[39m, in \u001b[36m_download_artifact_from_uri\u001b[39m\u001b[34m(artifact_uri, output_path, lineage_header_info)\u001b[39m\n\u001b[32m    100\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[33;03m    artifact_uri: The *absolute* URI of the artifact to download.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    105\u001b[39m \u001b[33;03m    lineage_header_info: The model lineage header info to be consumed by lineage services.\u001b[39;00m\n\u001b[32m    106\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    107\u001b[39m root_uri, artifact_path = _get_root_uri_and_artifact_path(artifact_uri)\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m repo = \u001b[43mget_artifact_repository\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[43m=\u001b[49m\u001b[43mroot_uri\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(repo, ModelsArtifactRepository):\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m repo.download_artifacts(\n\u001b[32m    112\u001b[39m         artifact_path=artifact_path,\n\u001b[32m    113\u001b[39m         dst_path=output_path,\n\u001b[32m    114\u001b[39m         lineage_header_info=lineage_header_info,\n\u001b[32m    115\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/mlflow/store/artifact/artifact_repository_registry.py:133\u001b[39m, in \u001b[36mget_artifact_repository\u001b[39m\u001b[34m(artifact_uri)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_artifact_repository\u001b[39m(artifact_uri: \u001b[38;5;28mstr\u001b[39m) -> ArtifactRepository:\n\u001b[32m    121\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[33;03m    Get an artifact repository from the registry based on the scheme of artifact_uri\u001b[39;00m\n\u001b[32m    123\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    131\u001b[39m \u001b[33;03m        requirements.\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_artifact_repository_registry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_artifact_repository\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/mlflow/store/artifact/artifact_repository_registry.py:77\u001b[39m, in \u001b[36mArtifactRepositoryRegistry.get_artifact_repository\u001b[39m\u001b[34m(self, artifact_uri)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m repository \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     73\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[32m     74\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not find a registered artifact repository for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00martifact_uri\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrently registered schemes are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m._registry.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     76\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrepository\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/mlflow/store/artifact/runs_artifact_repo.py:26\u001b[39m, in \u001b[36mRunsArtifactRepository.__init__\u001b[39m\u001b[34m(self, artifact_uri)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01martifact\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01martifact_repository_registry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_artifact_repository\n\u001b[32m     25\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(artifact_uri)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m uri = \u001b[43mRunsArtifactRepository\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_underlying_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mself\u001b[39m.repo = get_artifact_repository(uri)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/mlflow/store/artifact/runs_artifact_repo.py:39\u001b[39m, in \u001b[36mRunsArtifactRepository.get_underlying_uri\u001b[39m\u001b[34m(runs_uri)\u001b[39m\n\u001b[32m     37\u001b[39m (run_id, artifact_path) = RunsArtifactRepository.parse_runs_uri(runs_uri)\n\u001b[32m     38\u001b[39m tracking_uri = get_databricks_profile_uri_from_artifact_uri(runs_uri)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m uri = \u001b[43mget_artifact_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracking_uri\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m RunsArtifactRepository.is_runs_uri(uri)  \u001b[38;5;66;03m# avoid an infinite loop\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m add_databricks_profile_info_to_artifact_uri(uri, tracking_uri)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/mlflow/tracking/artifact_utils.py:52\u001b[39m, in \u001b[36mget_artifact_uri\u001b[39m\u001b[34m(run_id, artifact_path, tracking_uri)\u001b[39m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[32m     47\u001b[39m         message=\u001b[33m\"\u001b[39m\u001b[33mA run_id must be specified in order to obtain an artifact uri!\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     48\u001b[39m         error_code=INVALID_PARAMETER_VALUE,\n\u001b[32m     49\u001b[39m     )\n\u001b[32m     51\u001b[39m store = _get_store(tracking_uri)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m run = \u001b[43mstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Maybe move this method to RunsArtifactRepository so the circular dependency is clearer.\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m urllib.parse.urlparse(run.info.artifact_uri).scheme != \u001b[33m\"\u001b[39m\u001b[33mruns\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# avoid an infinite loop\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/mlflow/store/tracking/rest_store.py:177\u001b[39m, in \u001b[36mRestStore.get_run\u001b[39m\u001b[34m(self, run_id)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    168\u001b[39m \u001b[33;03mFetch the run from backend store\u001b[39;00m\n\u001b[32m    169\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    174\u001b[39m \u001b[33;03m    A single Run object if it exists, otherwise raises an Exception\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    176\u001b[39m req_body = message_to_json(GetRun(run_uuid=run_id, run_id=run_id))\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m response_proto = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGetRun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq_body\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Run.from_proto(response_proto.run)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/mlflow/store/tracking/rest_store.py:90\u001b[39m, in \u001b[36mRestStore._call_endpoint\u001b[39m\u001b[34m(self, api, json_body, endpoint)\u001b[39m\n\u001b[32m     88\u001b[39m     endpoint, method = _METHOD_TO_INFO[api]\n\u001b[32m     89\u001b[39m response_proto = api.Response()\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_host_creds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_proto\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/mlflow/utils/rest_utils.py:402\u001b[39m, in \u001b[36mcall_endpoint\u001b[39m\u001b[34m(host_creds, endpoint, method, json_body, response_proto, extra_headers)\u001b[39m\n\u001b[32m    399\u001b[39m     call_kwargs[\u001b[33m\"\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m\"\u001b[39m] = json_body\n\u001b[32m    400\u001b[39m     response = http_request(**call_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m response = \u001b[43mverify_rest_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    403\u001b[39m response_to_parse = response.text\n\u001b[32m    404\u001b[39m js_dict = json.loads(response_to_parse)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/mlflow/utils/rest_utils.py:259\u001b[39m, in \u001b[36mverify_rest_response\u001b[39m\u001b[34m(response, endpoint)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code != \u001b[32m200\u001b[39m:\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _can_parse_as_json_object(response.text):\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m RestException(json.loads(response.text))\n\u001b[32m    260\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    261\u001b[39m         base_msg = (\n\u001b[32m    262\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAPI request to endpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    263\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfailed with error code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m != 200\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    264\u001b[39m         )\n",
      "\u001b[31mRestException\u001b[39m: RESOURCE_DOES_NOT_EXIST: Run '9794c5f6d6dc41bf9ae605e2abf80364' not found"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct, col\n",
    "logged_model = 'runs:/9794c5f6d6dc41bf9ae605e2abf80364/ths_tune_model'\n",
    "\n",
    "# Load model as a Spark UDF. Override result_type if the model does not return double values.\n",
    "loaded_model =  mlflow.pyfunc.load_model(logged_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "print(x_train.shape)\n",
    "\n",
    "print(x_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 394us/step\n",
      "[[ -8.664396  -14.909999   -3.0911999 ...   8.796654   -8.077242\n",
      "   -6.6865964]\n",
      " [ -6.3493648 -11.766148    9.677436  ... -26.71225    -6.804982\n",
      "  -25.417377 ]\n",
      " [ -9.701519    4.1610136  -2.233385  ...  -2.4515321  -3.1858158\n",
      "   -6.341231 ]\n",
      " ...\n",
      " [-18.085354  -16.34156    -7.866031  ...  -2.9642308  -5.7162447\n",
      "   -1.7017349]\n",
      " [ -6.84557   -10.090346   -9.494044  ... -12.040836   -1.1177579\n",
      "  -11.440213 ]\n",
      " [ -5.868529  -17.192024   -4.854329  ... -22.456984  -11.155085\n",
      "  -16.662743 ]]\n"
     ]
    }
   ],
   "source": [
    "pred_ = loaded_model.predict(x_test)\n",
    "print(pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
